{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Defining the Task\n",
    "For this workshop we will:\n",
    "* Train GPT3.5 to automatically produce nutrition_logs, training_logs, and response messages in XML tags.\n",
    "* Our goal is to replace complex prompt instruction with simpler prompts against a specialized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare fine-tuning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate data set from larger model with more complex instructions\n",
    "\n",
    "> You are an expert personal trainer and fitness coach for athletes. Start by asking for a daily update, and you will be told what the user ate and trained that day. Respond with: 1. in <fitness_log> tags log the training (do not include dates) 2. in <nutrition_log> tags log the name of the food (do not include dates) 3. in <message> respond with \"Nice!\" Do not add any new line characters.\n",
    "\n",
    "* Then we simulated a few conversations and captured the responses in a list.\n",
    "* We collected a total of 20 samples for our task.\n",
    "* Fine-Tuning API requies a minimum of 10 samples for fine-tuning\n",
    "* OpenAI reports that a remarkable performance gain is observed when trainig examples go beyond 50 samples\n",
    "* 50 to 100 examples are typical\n",
    "* Exact number of required data samples depends on your specific use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an expert personal trainer and fitness coach for athletes. Start by asking for a daily update, and you will be told what the user ate and trained that day. Respond with: \n",
    "1. in <fitness_log> tags log the training (do not include dates)\n",
    "2. in <nutrition_log> tags log the name of the food (do not include dates)\n",
    "3. in <message> respond with \"Nice!\"\n",
    "\n",
    "Do not add any new line characters.'''\n",
    "\n",
    "user_prompts = [{\"role\": \"user\", \"content\": \"I ate eggs and ran half a mile\"}, \n",
    "                 {\"role\": \"user\", \"content\": \"I ate pizza and did 10 pushups\"}, \n",
    "                 {\"role\": \"user\", \"content\": \"I ate a salad and swam 5 laps\"}, \n",
    "                 {\"role\": \"user\", \"content\": \"I ate a burger and did 15 squats\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate cookies and did 20 crunches\"}, \n",
    "                 {\"role\": \"user\", \"content\": \"I ate a steak and did 30 burpees\"}, \n",
    "                 {\"role\": \"user\", \"content\": \"I ate a pie and did 5 pull-ups\"}, \n",
    "                 {\"role\": \"user\", \"content\": \"I ate cake and did 1 hour of yoga\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate soup and took a long walk\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate candy and did pilates\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate pasta and biked for 1 hour\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate sushi and hiked for 1 hour\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate cheese and did jumping jacks\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate shrimp and did Tae Kwon Do\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate cereal and did 20 sprints\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate chips and rowed for 1 hour\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate nuts and walked for 1 hour\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate tacos and ran 3 miles\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate a banana and did 50 pushups\"},\n",
    "                 {\"role\": \"user\", \"content\": \"I ate an orange and swam 10 laps\"},\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_role = {\"role\": \"system\", \"content\": \"You are a personal trainer and fitness coach.\"}\n",
    "conversations = []\n",
    "\n",
    "# loop through user prompts and generate responses\n",
    "for user_prompt in user_prompts:\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}, user_prompt],\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    seed=42\n",
    "  )\n",
    "\n",
    "  assistant_role = {\"role\": \"assistant\", \"content\": ...} # FILL IN\n",
    "\n",
    "  # build conversations collection\n",
    "  conversation = # FILL IN\n",
    "  # FILL IN\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Convert Data to .jsonl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view raw conversations\n",
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file_name = 'data.jsonl'\n",
    "\n",
    "# Write each conversation to the JSON Lines file line by line\n",
    "with open(jsonl_file_name, 'w') as file:\n",
    "    for conversation in conversations:\n",
    "        json.dump({\"messages\": conversation}, file)\n",
    "        # Add a newline to separate lines\n",
    "        # FILL IN \n",
    "\n",
    "print(f'The JSONL file has been written to {jsonl_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Verify Data and Evaluate Cost\n",
    "Here we perform following tasks:\n",
    "* Check if the data format is correct\n",
    "* All messages should be within max token limit of 4096\n",
    "* Statistical parameters of data (mean, median, max, min, P5, P95)\n",
    "* Count total number of tokens\n",
    "* Estimate the cost of fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UTILITY FUNCTION FROM AIUG DEVELOPER DAY\n",
    "\n",
    "# verify data utility\n",
    "def verify_data(data_path):\n",
    "\n",
    "    # Load dataset\n",
    "    with open(data_path) as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "    # Initial dataset stats\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    print(\"First example:\")\n",
    "    for message in dataset[0][\"messages\"]:\n",
    "        print(message)\n",
    "\n",
    "    # Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            if not content or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "\n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")\n",
    "\n",
    "    # Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "    # Token counting functions\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # not exact!\n",
    "    # simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "    def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += tokens_per_message\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "        num_tokens += 3\n",
    "        return num_tokens\n",
    "\n",
    "    def num_assistant_tokens_from_messages(messages):\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "        return num_tokens\n",
    "\n",
    "    def print_distribution(values, name):\n",
    "        print(f\"\\n#### Distribution of {name}:\")\n",
    "        print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "        print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "        print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "    # Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "    # Warnings and tokens counts\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "\n",
    "    for ex in dataset:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 16385 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "    # Pricing and default n_epochs estimate\n",
    "    MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "    MIN_TARGET_EXAMPLES = 10\n",
    "    MAX_TARGET_EXAMPLES = 25000\n",
    "    TARGET_EPOCHS = 3\n",
    "    MIN_EPOCHS = 1\n",
    "    MAX_EPOCHS = 25\n",
    "\n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    n_train_examples = len(dataset)\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "    print(f\"Based on current rate the cost of training is ~ $ {round(n_epochs * n_billing_tokens_in_dataset * 0.008 * 0.001,6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data file\n",
    "verify_data('data.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Upload Data to OpenAI Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_data_to_openai(file_path):\n",
    "    upload_response = client.files.create(\n",
    "    file=open(file_path, \"rb\"),\n",
    "    purpose= # FILL IN\n",
    "    )\n",
    "    \n",
    "    file_id = upload_response.id\n",
    "    print(f\"File upload complete. ID: {file_id}\")\n",
    "\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = # FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 & 7: Train and monitor a new fine-tuned model\n",
    "* We will recieve an email notification when the training job is complete\n",
    "* We can monitor progress on [OpenAI Portal under the finetuning tab](https://platform.openai.com/finetune).\n",
    "* We can also monitor progress using a [Weights and Biases](https://wandb.ai/site) integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_finetuning(file_id, n_epochs):\n",
    "  job = # FILL IN\n",
    "    training_file= # FILL IN\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    hyperparameters={\n",
    "          # FILL IN\n",
    "          }\n",
    "  )\n",
    "\n",
    "  job_id = job.id\n",
    "  print(f'Fine-tuning job successfully started: {job_id}')\n",
    "\n",
    "  return job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = # FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check status of fine-tuning job\n",
    "# FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Evaluating the fine-tuned model\n",
    "* Once the training job is complete, we can test our finetuned model using [OpenAI's Playground GUI](https://platform.openai.com/playground?mode=chat)\n",
    "* We can also use the fine-tuned model via the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility for testing your fine-tuned model\n",
    "def test_model(model_id, system_role, prompt):\n",
    "  response = client.chat.completions.create(\n",
    "  model=model_id,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"{system_role}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  return_message = response.choices[0].message.content\n",
    "\n",
    "  return return_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id can be obtained from https://platform.openai.com/finetune/\n",
    "model_id = # FILL IN\n",
    "\n",
    "# This is the same role used for training\n",
    "system_role = \"You are a personal trainer and fitness coach.\"\n",
    "\n",
    "# this is a test question\n",
    "prompt = # FILL IN\n",
    "\n",
    "# Test system Response\n",
    "response = test_model(\n",
    "    model_id,\n",
    "    system_role,\n",
    "    prompt\n",
    "    )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exercise 🚀\n",
    "\n",
    "* Fine-Tune GPT3.5 Turbo using OpenAI's Fine-Tuning API\n",
    "  * You can adjust the default behavior of myfitnessagent, add pre or post scripts, or change the formatting of the outputs.\n",
    "  * Generate your training dataset and save it in a JSONL format.\n",
    "  * Verify the data file, upload it, and start the fine-tuning job.\n",
    "  * Test the fine-tuned model to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
