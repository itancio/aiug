# Optimize AI deployment across multiple devices

## Summary

In this workshop, you will learn how to deploy AI models across different hardware, ensuring optimal performance from edge devices to cloud servers. We will explore how to use Intel OpenVinoâ€™s integration with Hugging Face to construct a RAG pipeline and apply model compression techniques to enhance efficiency without sacrificing performance.

## Key Functionality

The `quantization_rag.ipynb` notebook should be uploaded to google colab. It contains a step-by-step guide for OpenVino's model conversion and compression routines applied to embedding models, reranking models, and LLMs. The converted / compressed models are then applied to build a Langchain RAG Pipeline with a Gradio dashboard.
